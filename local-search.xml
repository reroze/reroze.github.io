<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>plan_solution</title>
    <link href="/2023/01/30/%E8%83%BD%E5%8A%9B%E6%8F%90%E5%8D%87%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/%E6%97%B6%E9%97%B4%E8%A7%84%E5%88%92/plan-solution/"/>
    <url>/2023/01/30/%E8%83%BD%E5%8A%9B%E6%8F%90%E5%8D%87%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/%E6%97%B6%E9%97%B4%E8%A7%84%E5%88%92/plan-solution/</url>
    
    <content type="html"><![CDATA[<h1 id="提升自己时间规划的能力"><a href="#提升自己时间规划的能力" class="headerlink" title="提升自己时间规划的能力"></a>提升自己时间规划的能力</h1><h2 id="1-时间块划分"><a href="#1-时间块划分" class="headerlink" title="1.时间块划分"></a>1.时间块划分</h2>]]></content>
    
    
    <categories>
      
      <category>growup</category>
      
      <category>planning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>时间规划能力提升方案</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>allennlp tokenizer</title>
    <link href="/2023/01/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/allennlp/TextFiled/TextField/"/>
    <url>/2023/01/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/allennlp/TextFiled/TextField/</url>
    
    <content type="html"><![CDATA[<h1 id="1-使用data-iterator生成iterator"><a href="#1-使用data-iterator生成iterator" class="headerlink" title="1.使用data_iterator生成iterator"></a>1.使用data_iterator生成iterator</h1>]]></content>
    
    
    <categories>
      
      <category>Allennlp</category>
      
      <category>TextField</category>
      
    </categories>
    
    
    <tags>
      
      <tag>blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>allennlp tokenizer</title>
    <link href="/2023/01/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/allennlp/batch-gen/batch-gen/"/>
    <url>/2023/01/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/allennlp/batch-gen/batch-gen/</url>
    
    <content type="html"><![CDATA[<h1 id="batch-中的数据的获取方式"><a href="#batch-中的数据的获取方式" class="headerlink" title="batch 中的数据的获取方式"></a>batch 中的数据的获取方式</h1><h2 id="1-Vocabulary"><a href="#1-Vocabulary" class="headerlink" title="1.Vocabulary"></a>1.Vocabulary</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">vocab = Vocabulary.from_params(config.pop(<span class="hljs-string">&#x27;vocabulary&#x27;</span>))<br></code></pre></td></tr></table></figure><p>config.yaml中的设置是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">vocabulary:<br>  <span class="hljs-built_in">type</span>: roberta<br>  directory_path: ./expt/vocabulary<br></code></pre></td></tr></table></figure><p>对应的程序代码位置在</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> copy<br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Dict</span>, Iterable, <span class="hljs-type">List</span>, <span class="hljs-type">Optional</span>, <span class="hljs-type">Set</span>, <span class="hljs-type">Union</span><br><br><span class="hljs-keyword">from</span> allennlp.data <span class="hljs-keyword">import</span> Vocabulary<br><span class="hljs-keyword">from</span> allennlp.data <span class="hljs-keyword">import</span> instance <span class="hljs-keyword">as</span> adi  <span class="hljs-comment"># pylint: disable=unused-import</span><br><span class="hljs-keyword">from</span> allennlp.data.vocabulary <span class="hljs-keyword">import</span> (DEFAULT_NON_PADDED_NAMESPACES,<br>                                      _NamespaceDependentDefaultDict)<br><span class="hljs-keyword">from</span> transformers.tokenization_bert <span class="hljs-keyword">import</span> load_vocab<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">_RobertaTokenToIndexDefaultDict</span>(<span class="hljs-params">_NamespaceDependentDefaultDict</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, non_padded_namespaces: <span class="hljs-type">Set</span>[<span class="hljs-built_in">str</span>], padding_token: <span class="hljs-built_in">str</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 oov_token: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-literal">None</span>:</span><br>        <span class="hljs-built_in">super</span>().__init__(non_padded_namespaces,<br>                         <span class="hljs-keyword">lambda</span>: &#123;padding_token: <span class="hljs-number">1</span>,<br>                                  oov_token: <span class="hljs-number">3</span>&#125;,<br>                         <span class="hljs-keyword">lambda</span>: &#123;&#125;)<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">_RobertaIndexToTokenDefaultDict</span>(<span class="hljs-params">_NamespaceDependentDefaultDict</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, non_padded_namespaces: <span class="hljs-type">Set</span>[<span class="hljs-built_in">str</span>], padding_token: <span class="hljs-built_in">str</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 oov_token: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-literal">None</span>:</span><br>        <span class="hljs-built_in">super</span>().__init__(non_padded_namespaces,<br>                         <span class="hljs-keyword">lambda</span>: &#123;<span class="hljs-number">1</span>: padding_token,<br>                                  <span class="hljs-number">3</span>: oov_token&#125;,<br>                         <span class="hljs-keyword">lambda</span>: &#123;&#125;)<br><br><br><span class="hljs-meta">@Vocabulary.register(<span class="hljs-params"><span class="hljs-string">&#x27;roberta&#x27;</span></span>)</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RobertaVocabulary</span>(<span class="hljs-params">Vocabulary</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Load vocabulary from a pre-trained Roberta vocab file.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    The __init__ is overwritten specifically so that we can match the</span><br><span class="hljs-string">    unknown and pad token with those in the pretrained Roberta vocab.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,</span></span><br><span class="hljs-params"><span class="hljs-function">                 counter: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">int</span>]] = <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 min_count: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 max_vocab_size: <span class="hljs-type">Union</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">int</span>]] = <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 non_padded_namespaces: Iterable[<span class="hljs-built_in">str</span>] = DEFAULT_NON_PADDED_NAMESPACES,</span></span><br><span class="hljs-params"><span class="hljs-function">                 <span class="hljs-comment">#DEFAULT_NON_PADDED_NAMESPACES = (&quot;*tags&quot;, &quot;*labels&quot;)</span></span></span><br><span class="hljs-params"><span class="hljs-function">                 pretrained_files: <span class="hljs-type">Optional</span>[<span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>]] = <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 only_include_pretrained_words: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 tokens_to_add: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]] = <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 min_pretrained_embeddings: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 padding_token: <span class="hljs-built_in">str</span> = <span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 oov_token: <span class="hljs-built_in">str</span> = <span class="hljs-string">&#x27;&lt;unk&gt;&#x27;</span></span>) -&gt; <span class="hljs-literal">None</span>:</span><br>        self._padding_token = padding_token <span class="hljs-comment">#默认是&#x27;&lt;pad&gt;&#x27;</span><br>        self._oov_token = oov_token <span class="hljs-comment">#默认是&#x27;&lt;unk&gt;&#x27;</span><br>        self._non_padded_namespaces = <span class="hljs-built_in">set</span>(non_padded_namespaces)<span class="hljs-comment">#</span><br>        self._token_to_index = _RobertaTokenToIndexDefaultDict(self._non_padded_namespaces,<br>                                                               self._padding_token,<br>                                                               self._oov_token)<br>        <span class="hljs-comment">#</span><br>        <br>        self._index_to_token = _RobertaIndexToTokenDefaultDict(self._non_padded_namespaces,<br>                                                               self._padding_token,<br>                                                               self._oov_token)<br>        <span class="hljs-comment">#生成空的token_to_index和index_to_token</span><br>        <br>        self._retained_counter: <span class="hljs-type">Optional</span>[<span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">int</span>]]] = <span class="hljs-literal">None</span><br>        <span class="hljs-comment"># Made an empty vocabulary, now extend it.</span><br>        self._extend(counter,<span class="hljs-comment">#None</span><br>                     min_count,<span class="hljs-comment">#None</span><br>                     max_vocab_size,<span class="hljs-comment">#None</span><br>                     non_padded_namespaces,<span class="hljs-comment">#(&quot;*tags&quot;, &quot;*labels&quot;)</span><br>                     pretrained_files,<span class="hljs-comment">#None</span><br>                     only_include_pretrained_words,<span class="hljs-comment">#False</span><br>                     tokens_to_add,<span class="hljs-comment">#None</span><br>                     min_pretrained_embeddings)<span class="hljs-comment">#None</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_load_bert_vocab</span>(<span class="hljs-params">self, vocab_path, namespace</span>):</span><br>        <span class="hljs-comment">#assert False</span><br>    <br>        vocab: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">int</span>] = load_vocab(vocab_path) <span class="hljs-comment">#尝试忽略会如何?</span><br>        <span class="hljs-keyword">for</span> word, idx <span class="hljs-keyword">in</span> vocab.items():<br>            <span class="hljs-keyword">try</span>:<br>                self._token_to_index[namespace][word] = idx<br>                self._index_to_token[namespace][idx] = word<br>            <span class="hljs-keyword">except</span>:<br>                <span class="hljs-built_in">print</span>(word, <span class="hljs-built_in">type</span>(word), idx)<br>                <span class="hljs-keyword">raise</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__setstate__</span>(<span class="hljs-params">self, state</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Conversely, when we unpickle, we need to reload the plain dicts</span><br><span class="hljs-string">        into our special DefaultDict subclasses.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># pylint: disable=attribute-defined-outside-init</span><br>        self.__dict__ = copy.copy(state)<br>        self._token_to_index = _RobertaTokenToIndexDefaultDict(self._non_padded_namespaces,<br>                                                               self._padding_token,<br>                                                               self._oov_token)<br>        self._token_to_index.update(state[<span class="hljs-string">&quot;_token_to_index&quot;</span>])<br>        self._index_to_token = _RobertaIndexToTokenDefaultDict(self._non_padded_namespaces,<br>                                                               self._padding_token,<br>                                                               self._oov_token)<br>        self._index_to_token.update(state[<span class="hljs-string">&quot;_index_to_token&quot;</span>])<br><br>        <span class="hljs-keyword">return</span> vocab<br><br></code></pre></td></tr></table></figure><p>model的forward_on_instances</p><p>会使用index_instances(self.vocab)</p><p>dataset.index_instances(self.vocab)</p><p>​    def index_instances(self, vocab: Vocabulary) -&gt; None:</p><p>​        for instance in self.instances:</p><p>​            instance.index_fields(vocab)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">index_fields</span>(<span class="hljs-params">self, vocab: Vocabulary</span>) -&gt; <span class="hljs-literal">None</span>:</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Indexes all fields in this ``Instance`` using the provided ``Vocabulary``.</span><br><span class="hljs-string">    This `mutates` the current object, it does not return a new ``Instance``.</span><br><span class="hljs-string">    A ``DataIterator`` will call this on each pass through a dataset; we use the ``indexed``</span><br><span class="hljs-string">    flag to make sure that indexing only happens once.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    This means that if for some reason you modify your vocabulary after you&#x27;ve</span><br><span class="hljs-string">    indexed your instances, you might get unexpected behavior.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.indexed:<br>        self.indexed = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">for</span> field <span class="hljs-keyword">in</span> self.fields.values():<br>            field.index(vocab)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@overrides</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">index</span>(<span class="hljs-params">self, vocab: Vocabulary</span>):</span><br>  token_arrays: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, TokenList] = &#123;&#125;<br>  indexer_name_to_indexed_token: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]] = &#123;&#125;<br>  token_index_to_indexer_name: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>] = &#123;&#125;<br>  <span class="hljs-keyword">for</span> indexer_name, indexer <span class="hljs-keyword">in</span> self._token_indexers.items():<br>    token_indices = indexer.tokens_to_indices(self.tokens, vocab, indexer_name) <span class="hljs-comment">#没有传入doc 因此是没有entity_mask的</span><br>    token_arrays.update(token_indices)<br>    indexer_name_to_indexed_token[indexer_name] = <span class="hljs-built_in">list</span>(token_indices.keys())<br>    <span class="hljs-keyword">for</span> token_index <span class="hljs-keyword">in</span> token_indices:<br>      token_index_to_indexer_name[token_index] = indexer_name<br>    self._indexed_tokens = token_arrays<br>    self._indexer_name_to_indexed_token = indexer_name_to_indexed_token<br>    self._token_index_to_indexer_name = token_index_to_indexer_name<br><br></code></pre></td></tr></table></figure><p>这里会调用indexer.tokens_to_indices</p>]]></content>
    
    
    <categories>
      
      <category>Allennlp</category>
      
      <category>dataflow</category>
      
      <category>getbatch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>allennlp tokenizer</title>
    <link href="/2023/01/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/allennlp/tokenindexer/tokenindexer/"/>
    <url>/2023/01/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/allennlp/tokenindexer/tokenindexer/</url>
    
    <content type="html"><![CDATA[<h1 id="Allennlp-0-9-0中的word-tokenindex"><a href="#Allennlp-0-9-0中的word-tokenindex" class="headerlink" title="Allennlp 0.9.0中的word_tokenindex"></a>Allennlp 0.9.0中的word_tokenindex</h1><p>这里以使用roberta为例</p><p>roberta的word_indexer</p><h2 id="1-init-py"><a href="#1-init-py" class="headerlink" title="1._init_.py"></a>1._<em>init_</em>.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@TokenIndexer.register(<span class="hljs-params"><span class="hljs-string">&quot;roberta&quot;</span></span>)</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RobertaTokenIndexer</span>(<span class="hljs-params">TokenIndexer[<span class="hljs-built_in">int</span>]</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,</span></span><br><span class="hljs-params"><span class="hljs-function">                 model_name: <span class="hljs-built_in">str</span> = <span class="hljs-string">&#x27;roberta-base&#x27;</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 namespace: <span class="hljs-built_in">str</span> = <span class="hljs-string">&#x27;bpe&#x27;</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 legacy: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 start_tokens: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 end_tokens: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 token_min_padding_length: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 padding_on_right: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 padding_value: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 max_len: <span class="hljs-built_in">int</span> = <span class="hljs-number">512</span></span>) -&gt; <span class="hljs-literal">None</span>:</span><br>        <span class="hljs-built_in">super</span>().__init__(token_min_padding_length)<br>        <span class="hljs-comment">#roberta = torch.hub.load(&#x27;pytorch/fairseq:2f7e3f3323&#x27;, &#x27;bart.large.xsum&#x27;)</span><br>        roberta = torch.hub.load(<span class="hljs-string">&#x27;pytorch/fairseq:2f7e3f3323&#x27;</span>, <span class="hljs-string">&#x27;roberta.base&#x27;</span>)<br>        self.source_dictionary = roberta.task.source_dictionary<br>        <span class="hljs-comment">#调用的是fairseq的包</span><br>        self.bpe = roberta.bpe.bpe<br>        self.bpe_legacy = roberta.bpe<br>        self._added_to_vocabulary = <span class="hljs-literal">False</span><br>        self._namespace = namespace<br>        self._padding_on_right = padding_on_right<br>        self._padding_value = padding_value<br>        self._max_len = max_len<br>        self.legacy = legacy<br></code></pre></td></tr></table></figure><p>使用roberta.base作为bpe的词典，roberta.base和robert.large的词典是一样的</p>]]></content>
    
    
    <categories>
      
      <category>Allennlp</category>
      
      <category>tokenindexer</category>
      
    </categories>
    
    
    <tags>
      
      <tag>blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>allennlp tokenizer</title>
    <link href="/2023/01/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/allennlp/tokenizer/tokenizer/"/>
    <url>/2023/01/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/allennlp/tokenizer/tokenizer/</url>
    
    <content type="html"><![CDATA[<h1 id="在Allennlp-0-9中使用的tokenizer"><a href="#在Allennlp-0-9中使用的tokenizer" class="headerlink" title="在Allennlp 0.9中使用的tokenizer"></a>在Allennlp 0.9中使用的tokenizer</h1><h2 id="1-tokenizer"><a href="#1-tokenizer" class="headerlink" title="1.tokenizer"></a>1.tokenizer</h2><p>tokenizer使用的是word类型</p><p>在config.yaml中的设置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">tokenizer:<br>  <span class="hljs-built_in">type</span>: word<br>  word_splitter: just_spaces_keep_newlines<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#/home/zhangjunzhe/anaconda3/envs/tell_apex/lib/python3.7/site-packages/allennlp/data/tokenizers/word_tokenizer.py</span><br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">List</span><br><br><span class="hljs-keyword">from</span> overrides <span class="hljs-keyword">import</span> overrides<br><br><span class="hljs-keyword">from</span> allennlp.data.tokenizers.token <span class="hljs-keyword">import</span> Token<br><span class="hljs-keyword">from</span> allennlp.data.tokenizers.tokenizer <span class="hljs-keyword">import</span> Tokenizer<br><span class="hljs-keyword">from</span> allennlp.data.tokenizers.word_filter <span class="hljs-keyword">import</span> WordFilter, PassThroughWordFilter<br><span class="hljs-keyword">from</span> allennlp.data.tokenizers.word_splitter <span class="hljs-keyword">import</span> WordSplitter, SpacyWordSplitter<br><span class="hljs-keyword">from</span> allennlp.data.tokenizers.word_stemmer <span class="hljs-keyword">import</span> WordStemmer, PassThroughWordStemmer<br><br><br><span class="hljs-meta">@Tokenizer.register(<span class="hljs-params"><span class="hljs-string">&quot;word&quot;</span></span>)</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordTokenizer</span>(<span class="hljs-params">Tokenizer</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    A ``WordTokenizer`` handles the splitting of strings into words as well as any desired</span><br><span class="hljs-string">    post-processing (e.g., stemming, filtering, etc.).  Note that we leave one particular piece of</span><br><span class="hljs-string">    post-processing for later: the decision of whether or not to lowercase the token.  This is for</span><br><span class="hljs-string">    two reasons: (1) if you want to make two different casing decisions for whatever reason, you</span><br><span class="hljs-string">    won&#x27;t have to run the tokenizer twice, and more importantly (2) if you want to lowercase words</span><br><span class="hljs-string">    for your word embedding, but retain capitalization in a character-level representation, we need</span><br><span class="hljs-string">    to retain the capitalization here.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    word_splitter : ``WordSplitter``, optional</span><br><span class="hljs-string">        The :class:`WordSplitter` to use for splitting text strings into word tokens.  The default</span><br><span class="hljs-string">        is to use the ``SpacyWordSplitter`` with default parameters.</span><br><span class="hljs-string">    word_filter : ``WordFilter``, optional</span><br><span class="hljs-string">        The :class:`WordFilter` to use for, e.g., removing stopwords.  Default is to do no</span><br><span class="hljs-string">        filtering.</span><br><span class="hljs-string">    word_stemmer : ``WordStemmer``, optional</span><br><span class="hljs-string">        The :class:`WordStemmer` to use.  Default is no stemming.</span><br><span class="hljs-string">    start_tokens : ``List[str]``, optional</span><br><span class="hljs-string">        If given, these tokens will be added to the beginning of every string we tokenize.</span><br><span class="hljs-string">    end_tokens : ``List[str]``, optional</span><br><span class="hljs-string">        If given, these tokens will be added to the end of every string we tokenize.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,</span></span><br><span class="hljs-params"><span class="hljs-function">                 word_splitter: WordSplitter = <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 word_filter: WordFilter = PassThroughWordFilter(<span class="hljs-params"></span>),</span></span><br><span class="hljs-params"><span class="hljs-function">                 word_stemmer: WordStemmer = PassThroughWordStemmer(<span class="hljs-params"></span>),</span></span><br><span class="hljs-params"><span class="hljs-function">                 start_tokens: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                 end_tokens: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span></span>) -&gt; <span class="hljs-literal">None</span>:</span><br>        self._word_splitter = word_splitter <span class="hljs-keyword">or</span> SpacyWordSplitter()<br>        self._word_filter = word_filter<br>        self._word_stemmer = word_stemmer<br>        self._start_tokens = start_tokens <span class="hljs-keyword">or</span> []<br>        <span class="hljs-comment"># We reverse the tokens here because we&#x27;re going to insert them with `insert(0)` later;</span><br>        <span class="hljs-comment"># this makes sure they show up in the right order.</span><br>        self._start_tokens.reverse()<br>        self._end_tokens = end_tokens <span class="hljs-keyword">or</span> []<br><br><span class="hljs-meta">    @overrides</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tokenize</span>(<span class="hljs-params">self, text: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-type">List</span>[Token]:</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Does whatever processing is required to convert a string of text into a sequence of tokens.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        At a minimum, this uses a ``WordSplitter`` to split words into text.  It may also do</span><br><span class="hljs-string">        stemming or stopword removal, depending on the parameters given to the constructor.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        words = self._word_splitter.split_words(text)<br>        <span class="hljs-keyword">return</span> self._filter_and_stem(words)<br><br><span class="hljs-meta">    @overrides</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">batch_tokenize</span>(<span class="hljs-params">self, texts: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[Token]]:</span><br>        batched_words = self._word_splitter.batch_split_words(texts)<br>        <span class="hljs-keyword">return</span> [self._filter_and_stem(words) <span class="hljs-keyword">for</span> words <span class="hljs-keyword">in</span> batched_words]<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_filter_and_stem</span>(<span class="hljs-params">self, words: <span class="hljs-type">List</span>[Token]</span>) -&gt; <span class="hljs-type">List</span>[Token]:</span><br>        filtered_words = self._word_filter.filter_words(words)<br>        stemmed_words = [self._word_stemmer.stem_word(word) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> filtered_words]<br>        <span class="hljs-keyword">for</span> start_token <span class="hljs-keyword">in</span> self._start_tokens:<br>            stemmed_words.insert(<span class="hljs-number">0</span>, Token(start_token, <span class="hljs-number">0</span>))<br>        <span class="hljs-keyword">for</span> end_token <span class="hljs-keyword">in</span> self._end_tokens:<br>            stemmed_words.append(Token(end_token, -<span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">return</span> stemmed_words<br><br></code></pre></td></tr></table></figure><p>tokenizer 调用tokenize部分 分别调用_word_splitter.split_(words)和_filter_and_stem(words)</p><h2 id="1-1-wordsplitter-split-words"><a href="#1-1-wordsplitter-split-words" class="headerlink" title="1.1 _wordsplitter.split\(words)"></a>1.1 _word<em>splitter.split\</em>(words)</h2><p>Tell的_word<em>splitter.split\</em>(words)的方式是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">List</span><br><br><span class="hljs-keyword">from</span> allennlp.data.tokenizers.token <span class="hljs-keyword">import</span> Token<br><span class="hljs-keyword">from</span> allennlp.data.tokenizers.word_splitter <span class="hljs-keyword">import</span> WordSplitter<br><span class="hljs-keyword">from</span> overrides <span class="hljs-keyword">import</span> overrides<br><br><br><span class="hljs-meta">@WordSplitter.register(<span class="hljs-params"><span class="hljs-string">&#x27;just_spaces_keep_newlines&#x27;</span></span>)</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">JustSpacesKeepNewlinesWordSplitter</span>(<span class="hljs-params">WordSplitter</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    A ``WordSplitter`` that assumes you&#x27;ve already done your own tokenization somehow and have</span><br><span class="hljs-string">    separated the tokens by spaces.  We just split the input string on whitespace and return the</span><br><span class="hljs-string">    resulting list.  We use a somewhat odd name here to avoid coming too close to the more</span><br><span class="hljs-string">    commonly used ``SpacyWordSplitter``.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Note that we use ``sentence.split()``, which means that the amount of whitespace between the</span><br><span class="hljs-string">    tokens does not matter.  This will never result in spaces being included as tokens.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><span class="hljs-meta">    @overrides</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">split_words</span>(<span class="hljs-params">self, sentence: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-type">List</span>[Token]:</span><br>        <span class="hljs-keyword">return</span> [Token(t) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> sentence.split(<span class="hljs-string">&#x27; &#x27;</span>)]<br><br></code></pre></td></tr></table></figure><p>也就是简单的[Token(t) for t in sentence.split(‘ ‘)]按照空格分开</p><h2 id="1-2-filter-and-stem-words"><a href="#1-2-filter-and-stem-words" class="headerlink" title="1.2 _filter_and_stem(words)"></a>1.2 _filter_and_stem(words)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#没有设置 使用的是原版的</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_filter_and_stem</span>(<span class="hljs-params">self, words: <span class="hljs-type">List</span>[Token]</span>) -&gt; <span class="hljs-type">List</span>[Token]:</span><br>  filtered_words = self._word_filter.filter_words(words)<br>  stemmed_words = [self._word_stemmer.stem_word(word) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> filtered_words]<br><span class="hljs-comment">#start_tokens: List[str] = None,</span><br><span class="hljs-comment">#end_tokens: List[str] = None) -&gt; None: 默认是为None</span><br>  <span class="hljs-keyword">for</span> start_token <span class="hljs-keyword">in</span> self._start_tokens:<br>stemmed_words.insert(<span class="hljs-number">0</span>, Token(start_token, <span class="hljs-number">0</span>))<br>  <span class="hljs-keyword">for</span> end_token <span class="hljs-keyword">in</span> self._end_tokens:<br>    stemmed_words.append(Token(end_token, -<span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">return</span> stemmed_words<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">_word_filter.filter_words(words)<br><span class="hljs-comment">#WordFilter = PassThroughWordFilter()</span><br><span class="hljs-meta">@WordFilter.register(<span class="hljs-params"><span class="hljs-string">&#x27;pass_through&#x27;</span></span>)</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PassThroughWordFilter</span>(<span class="hljs-params">WordFilter</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Does not filter words; it&#x27;s a no-op.  This is the default word filter.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><span class="hljs-meta">    @overrides</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">filter_words</span>(<span class="hljs-params">self, words: <span class="hljs-type">List</span>[Token]</span>) -&gt; <span class="hljs-type">List</span>[Token]:</span><br>        <span class="hljs-keyword">return</span> words<br><br><span class="hljs-comment">#默认的是不进行filter</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@WordStemmer.register(<span class="hljs-params"><span class="hljs-string">&#x27;pass_through&#x27;</span></span>)</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PassThroughWordStemmer</span>(<span class="hljs-params">WordStemmer</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Does not stem words; it&#x27;s a no-op.  This is the default word stemmer.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><span class="hljs-meta">    @overrides</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">stem_word</span>(<span class="hljs-params">self, word: Token</span>) -&gt; Token:</span><br>        <span class="hljs-keyword">return</span> word<br><span class="hljs-comment">#默认也是不进行stem</span><br><br></code></pre></td></tr></table></figure><p>在datareader中的使用方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">context_tokens = self._tokenizer.tokenize(context)<br>caption_tokens = self._tokenizer.tokenize(caption)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#tell中的context的使用方式</span><br><span class="hljs-number">1.</span>从数据集中加载对应的article<br><span class="hljs-number">2.</span>context = <span class="hljs-string">&#x27; &#x27;</span>.join(context.strip().split(<span class="hljs-string">&#x27; &#x27;</span>)[:<span class="hljs-number">500</span>])<br><span class="hljs-number">3.</span>context_tokens = self._tokenizer.tokenize(context)<br><span class="hljs-number">4.</span>TextField(context_tokens, self._token_indexers)<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Allennlp</category>
      
      <category>tokenizer</category>
      
    </categories>
    
    
    <tags>
      
      <tag>blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>typora常用符号</title>
    <link href="/2022/06/11/%E5%8D%9A%E5%AE%A2/typora%E5%B8%B8%E7%94%A8%E7%AC%A6%E5%8F%B7/"/>
    <url>/2022/06/11/%E5%8D%9A%E5%AE%A2/typora%E5%B8%B8%E7%94%A8%E7%AC%A6%E5%8F%B7/</url>
    
    <content type="html"><![CDATA[<p><img src="/2022/06/11/%E5%8D%9A%E5%AE%A2/typora%E5%B8%B8%E7%94%A8%E7%AC%A6%E5%8F%B7/figure1.png" alt></p><p><img src="/2022/06/11/%E5%8D%9A%E5%AE%A2/typora%E5%B8%B8%E7%94%A8%E7%AC%A6%E5%8F%B7/2.png" alt></p><p><img src="/2022/06/11/%E5%8D%9A%E5%AE%A2/typora%E5%B8%B8%E7%94%A8%E7%AC%A6%E5%8F%B7/3.png" alt></p><p><img src="/2022/06/11/%E5%8D%9A%E5%AE%A2/typora%E5%B8%B8%E7%94%A8%E7%AC%A6%E5%8F%B7/4.png" alt="4"></p><p><img src="/2022/06/11/%E5%8D%9A%E5%AE%A2/typora%E5%B8%B8%E7%94%A8%E7%AC%A6%E5%8F%B7/5.png" alt="5"></p><p><img src="/2022/06/11/%E5%8D%9A%E5%AE%A2/typora%E5%B8%B8%E7%94%A8%E7%AC%A6%E5%8F%B7/6.png" alt="6"></p><p><img src="/2022/06/11/%E5%8D%9A%E5%AE%A2/typora%E5%B8%B8%E7%94%A8%E7%AC%A6%E5%8F%B7/7.png" alt="7"></p><p><img src="/2022/06/11/%E5%8D%9A%E5%AE%A2/typora%E5%B8%B8%E7%94%A8%E7%AC%A6%E5%8F%B7/8.png" alt="8"></p><p><img src="/2022/06/11/%E5%8D%9A%E5%AE%A2/typora%E5%B8%B8%E7%94%A8%E7%AC%A6%E5%8F%B7/9.png" alt="9"></p><p><img src="/2022/06/11/%E5%8D%9A%E5%AE%A2/typora%E5%B8%B8%E7%94%A8%E7%AC%A6%E5%8F%B7/10.png" alt="10"></p><p>转载于<a href="https://blog.csdn.net/SIMBA1949/article/details/79001226">https://blog.csdn.net/SIMBA1949/article/details/79001226</a></p>]]></content>
    
    
    <categories>
      
      <category>Write</category>
      
      <category>Typora</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Write, Typora</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NMT-with-Monolingual-Translation-Memory</title>
    <link href="/2022/06/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/NMT-with-Monolingual-Translation-Memory/"/>
    <url>/2022/06/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/NMT-with-Monolingual-Translation-Memory/</url>
    
    <content type="html"><![CDATA[<h1 id="Neural-Machine-Translation-with-Monolingual-Translation-Memory"><a href="#Neural-Machine-Translation-with-Monolingual-Translation-Memory" class="headerlink" title="Neural Machine Translation with Monolingual Translation Memory"></a><strong>Neural Machine Translation with Monolingual Translation Memory</strong></h1><p>TM(Translate memory)可以增加NMT的翻译效果。</p><p>TM的定义：</p><p> 与之前使用双语语料和源语言端的相似度来查找记忆检索的方法不同，本方法提出了一个新的框架，使用单语记忆在跨语种的规则上表现出了可学习性的记忆检索。</p><p>优点1：跨语言的记忆检索可以让大量的单语料信息成为TM(翻译记忆)</p><p>优点2：记忆检索模型和NMT模型可以以最后的翻译目标一起被优化。</p><p>本框架的实验效果有着很大的提升，甚至比一些使用双语语料的翻译以及的翻译模型效果还要好。由于本框架对于单语语料信息的使用，也证明 了本框架在一些低来源和不同领域的适用性的效果。</p><p>参数神经网络伴随非参数记忆已经成为了一个有潜力的缓解更大模型规模发展的方向。Translation Memory（Guoping Huang, Lemao Liu, Xing Wang, Longyue Wang, Huayang Li, Zhaopeng Tu, Chengyan Huang, and Shuming Shi. 2021. Transmart: a practical in teractive machine translation system. <em>arXiv preprint</em> <em>arXiv</em>. ）</p><p>本框架中的，一个有参数的神经机器翻译模型和单语料的翻译记忆和一个可学习的跨语种记忆检索器。</p><p>将源语言与目标语言嵌入到同一片浅层向量空间中。dual-encoder framework</p><p>(Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Sackinger, and Roopak Shah. 1993. Signature veri-fification using a” siamese” time delay neural network. In <em>Proceedings of the 6th International Conference</em> <em>on Neural Information Processing Systems</em>, pages 737–744.)</p><p>记忆检索器，直接将源语言的输入和目标语言端的点相连接。这样，可以使得在目标语言端的单语数据也可以作为TM（翻译记忆来使用）</p><p>整个翻译过程可以分为两个部分，1.检索 2.生成 </p><p>对于一个输入x，从目标语言端的数据（除了对应目标语言的数据集外，还包含其他方面的数据集，只要是目标语言端的就可以）中选择M条目标语言语句，${z<em>i}</em>{i=1}^{M}$  并计算input与检索出来的目标语言端的语句计算相关分数 $f(x,z_i)$ 之后通过输入的句子x和挑选出的目标语言端的语句z以及计算的相关关系来生成最后的翻译语句。 $p(y|x, z_1, f(x, z_1), z_2, f(x, z_2), …, z_M, f(x, z_M))$  由于相关性分数$f(x, z_i)$ 也作为模型最后翻译的输入，因此，模型会同时训练memory retrieval模型和翻译模型。</p><p>记忆检索模型，将包括翻译数据集目标语言语句在内的目标语言语料均作为翻译记忆，检索模型可能对于每一个输入需要计算所有的目标语言与其的相关分数。计算量较大。 </p><p>作者最后采用了dual-encoder框架来解决此问题，此框架下，寻找最相关的句子可以被简化成Maximum Inner Product Search （MIPS）问题。</p><p>计算$f(x, z)=E<em>{src}(x)^TE</em>{tgt}(z)$ 即将输入与翻译记忆编码后的嵌入向量进行点积即可进行分数的计算。</p><p>$E<em>{src}与E</em>{tgt}$ 都是编码网络，将输入和目标语言文本映射到d维向量空间。作者使用了两个独立的Transformer结构来进行编码器的设置。</p><p>$E<em>{src}(x) = normalize(W</em>{src}Trans_{src}(x))$</p><p>$E<em>{tgt}(z) = normalize(W</em>{tgt}Trans_{tgt}(x))$ </p><p>本框架使用自然回归解码的方式进行，框架为了融入记忆信息，增加了memory-encoder和cross-attn with memory encoder output。</p><p>$\alpha<em>{ij} = \frac{exp(h_t^TW_mZ</em>{i,j})}{\sum\limits<em>{i=1}^M\sum\limits</em>{k=1}^{L<em>i}exp(h_t^TW_mz</em>{i,k})}$   </p><p>先正常计算self-attn和src-attn，然后进行retriev从目标语句空间中检索出M个目标语句，并将检索分数作为计算attn的bias来计算原始decoder输出与memory的cross-mem-attn 。最后使用最终的值生成模型的预测解，并以gate的概率使用生成的解或者mem中对应概率的单词。</p><p>检索模型的预训练任务1.随机抽取一部分源语言，目标语言对，然后使用检索模型挑选为其中源语言挑选正确的目标语言语句。</p><p>检索模型的预训练任务2，是通过使用源语言的表征来预测目标语言的单词，通过目标语言的表征来预测源语言的单词</p>]]></content>
    
    
    <categories>
      
      <category>NMT</category>
      
      <category>Memory</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NMT, Memory</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo使用手册</title>
    <link href="/2022/06/06/%E5%8D%9A%E5%AE%A2/hexo%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"/>
    <url>/2022/06/06/%E5%8D%9A%E5%AE%A2/hexo%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/</url>
    
    <content type="html"><![CDATA[<h2 id="Hexo新建"><a href="#Hexo新建" class="headerlink" title="Hexo新建"></a>Hexo新建</h2><p>使用hexo new ‘filename’ 来新建 会在_post文件夹下新建一个文件和对应的图文件夹</p><h2 id="Hexo指令"><a href="#Hexo指令" class="headerlink" title="Hexo指令"></a>Hexo指令</h2><p>hexo s 本地预览</p><p>hexo d 部署</p><p>hexo g 生成新的改动</p>]]></content>
    
    
    <categories>
      
      <category>Begin</category>
      
      <category>blog_learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>lifelong-learing-review</title>
    <link href="/2021/08/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/lifelong-learing-review/"/>
    <url>/2021/08/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/lifelong-learing-review/</url>
    
    <content type="html"><![CDATA[<h1 id="Continual-Lifelong-Learning-with-Neural-Networks-A-Review"><a href="#Continual-Lifelong-Learning-with-Neural-Networks-A-Review" class="headerlink" title="Continual Lifelong Learning with Neural Networks:*A Review"></a><strong>Continual Lifelong Learning with Neural Networks:*A Review</strong></h1><p>在真实世界的信息流下：Lifelong learning对计算机系统和自行交互系统很重要。然而不论是机器学习还是神经网络，lifelong learning都存在一个长期存在的问题，由于存在长期的可以从不稳定的数据分布中获得数据，通常会导致灾难性的遗忘或者冲突。这对state of art的神经网络模型不利，因为神经网络是从训练数据中以固定大小的batch抽取数据进行训练的，并没有考虑到数据是一个递增出现的情况。这篇文章详细的列出了在人工智能系统中和lifelong learning紧密相连的一些挑战，并且比较了现有的一些神经网络尝试减轻在不同的范围中的灾难性遗忘。尽管AI已经在特定的域中获得了显著的成就，大量的研究被要求在自动化的终端或者机器上发展稳定的lifelong学习系统。我们讨论了有效建立的和重要的研究，这些研究被生物系统的lifelong learning所驱动，例如结构性重构，记忆再现，课程学习和迁移学习，内在动机，多感官集成。</p><h2 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h2><p>​    The ability to continually learn over time by accommodating new knowledge while retaining previously learned experiences is referred to as continual or lifelong learning.</p><p>​    主流的计算模型认为lifelong learning非常容易灾难性遗忘和灾难性干扰。(McClelland et al. 1995, McCloskey &amp; Cohen 1989)在原有训练好的模型上进行新的模型训练，这个现象很容易导致模型的表现下降，在最糟的情况下，<code>old knowledge</code>会被<code>new knowledge</code><strong>完全覆写</strong>。</p><p>​    目前神经网络的训练方法依赖于在训练采样中的大batch，但是这样的方式需要假设所有的sample需要在training分割可用，因此在每一个batch中都需要对网络进行再训练，在每一个dataset中都需要对网络进行再次的训练，以此来适应数据集上数据的分布变化。（ Guo et al. (2016), LeCun et al. (2015)）</p><p>​    当在序列化任务上进行训练时，传统的神经网络模型在学习当前的任务时，之前的任务中表现显著下降。(Kemker et al. 2018, Maltoni &amp; Lomonaco 2018). 尽管$\textcolor{red}{模型再训练}$的方法从实用的角度上能解决特定数据集的一部分问题，但是这种方法是非常低效且无法在现实中学习新的数据。例如，在情景学习中，可能$\textcolor{red}{不会有明显的训练集测试集的区分}$，并需要实时的对对应的行为回复进行适应。(Cangelosi &amp; Schlesinger 2015, Tani 2016).</p><p>​    为了克服灾难性遗忘，学习系统必须一方面获取新的知识，并根据持续的输入进行之前学习到的知识的精炼，另一方面需要避免已经存在的知识对当前的输入造成明显的干扰。（==思考：因此或许可以使用两个网络？==）。<strong>stability-plasticity dilemma</strong>，是指一个系统需要是可塑的，为了整合新输入的信息，同时还要稳定，为了不产生灾难性的干扰现象。(Ditzler et al. 2015, Mermillod et al. 2013, Grossberg 1980, 2012).由于lifelong learning的挑战性和高影响性，大量的计算的方法开始被提出，并且被哺乳动物的大脑在学习时的生物学上的因素所启发。</p><hr><p>​    接下来会介绍，一些被研究的生理学上的研究以及他们对建立lifelong learning模型的启发，首先，我们关注神经结构上的可塑性的机制，在多种大脑中的可塑-稳定平衡的调节机制即（<strong>stability-plasticity dilemma</strong>）.可塑性是一个重要的方面，不论是对生物还是脑回路而言，对于一个稳定的持续的lifelong过程，两种可塑性是需要的</p><p><strong>（1）</strong> Hebbian plasticity用来构建不稳定的正反馈，</p><p><strong>（2）</strong> 用来补偿稳定的负反馈机制。</p><p>​    已经有实验观察到，有些特别的机制来处理新的任务在遇到干扰时，保护之前任务中学习到的知识，通过降低突触的学习率来实现。同时Hebbian还发现，一个自适应的可塑性稳定了网络的回路，从而形成了一个最佳的经验驱动的网络连接，集合和功能。</p><p>​    大脑需要执行两个互补的任务。==1.在经验上总结概括，2.和保持住暂时的记忆==。在2.4节我们会介绍互补学习系统<strong>complementary learning systems(CLS) (McClelland et al. 1995, Kumaran et al. 2016) </strong> 这个模型holds the means for有效的从之前的时间中提取出统计结构，并保持住暂时的记忆（也就是对一个特定的时间和地点的经历的总结收集）。</p><p>​    海马体和大脑皮层的互补作用使得人体获得一个相对稳定统一的知识系统，海马体具有短期的适应能力，并且具有新知识的$\textcolor{red}{快速学习能力}$，而所学习到的新知识又会被迁移和融合到大脑皮层中作为长期的知识储备。（==短期和长期的网络？ 在特定任务上主要使用短期的网络结合长期网络进行测试，每次训练后将获得的知识去除数据偏移后总结归纳给长期网络 （auto DL？）==） 而大脑皮层的$\textcolor{red}{学习率很低}$，并且主要负责总结和概括。然而，即使是对人脑而言，灾难性遗忘有时还是会发生。</p><p>​    在生物神经网络<strong>lifelong learning</strong>方面上的研究已经启发了许多ML和DL的方法，在第三节我们会介绍和比较用来解决或者缓解灾难性遗忘的方法。主要关注以下几种模型：</p><p><strong>(1)</strong> 调节固有的突触可塑性（==调整学习率？==），来维护保护统一的知识体系。（in Sec</p><p>3.2）</p><p><strong>(2)</strong> 分配额外的网络资源来学习新的信息。（Sec。3.3）</p><p><strong>(3)</strong> 使用互补学习系统来实现统一考虑的记忆已经重现经历。（Sec. 3.4）</p><p>​    这些系统大部分都是在$\textcolor{red}{有限的有注解的数据集上进行有监督的lifelong学习}$，并没有拓展到新的复杂的场景，例如对<strong>部分无标记的数据序列的处理</strong>(==单语机器翻译?==)。 无监督的lifelong learning一方面已经被<strong>self-organizing neural networks</strong> 提及<strong>(e.g., Parisi, Tani, Weber &amp; Wermter (2018, 2017), Richardson &amp; Thomas (2008))</strong> </p><p>​    尽管在设计带有结构正则化(<strong>structural regularization</strong>)或者动态结构更新(<strong>dynamic architectural update</strong>)的学习方法有了显著的提升，但是考虑到很少的注意力被放在怎样严格的==评估==这些算法在终身和递增的学习任务中的表现。因此，在Sec 3.5我们讨论了$\textcolor{red}{设计和使用一定量的指标来衡量大规模的数据集上的灾难性遗忘的程度的重要性}$。</p><hr><p>由于lifelong learning在自主学习的终端和机器人上的意义，其在受到越来越多的关注。    神经网络被特别的设计成了$\textcolor{red}{增量适应}$特别的形式通常是人工标注的$\textcolor{red}{在受限制的环境}$中进行的数据采样，彼此==顺序随机== （==神经网络不同数据集训练的顺序，或者同一个数据集的不同batch训练的顺序会对结果产生影响吗？==）且独立。而这个与人类和其他生物在其生命中的生理上的学习环境有$\textcolor{red}{显著的区别}$。在现实生活中的终端必须要能够处理不确定的知觉，能够有效不断的处理多种感官上的信息流，并且能够在之前学习到的基础上学习新的一系列任务并且不发生灾难性干扰。直觉上，这里存在一个巨大的鸿沟于上述提到的神经网络模型和目标对应的一些尝试使用不断的感知经验进行增量学习的更复杂的终身学习系统。</p><p>​    人类可以获得新的技能并且在不同的域和任务中迁移知识，然而人工智能系统还是仍然只能达到其初期的阶段，这个被称为神经网络中的<strong>transfer learning</strong>。<strong>(Weiss et al. 2016)</strong> 并且相较于主流的使用$\textcolor{red}{单一的感觉方面的信息}$训练神经网络的趋势而言，大脑更希望获得多种感官上的信息。（==多感官信息机器翻译？==） 并且这也在感觉不确定的情况下提供了一个有效的交互手段。多感官在大脑早期的发育和感知器的特化中的作用启发了许多研究的自治系统。在Sec4，我们重温了计算机上的被生物学习方面所启发的包括决定性的发展阶段和课程学习，在学习新的任务中，再次使用知识进行迁移学习，强化学习在自监督下自动的探索新的环境。和多感官的跨模型学习。</p><p>​    这篇综述在灾难性遗忘上在<code>connectionist models</code>上进行了补充，没有精细的比较最近的实验和针对在实验中已经观察的的机制，怎样训练和评估lifelong 方法定义清晰的指导方针。lifelong learning是一个跨学科的挑战性的问题。第5节，我们会总结出来展示出来的关键idea并提供一个正在进行和未来的研究方向。</p><p>==（生物学上的启发？）学习新知识到学习就知识？==</p><p>==（统计学上的启发？） 一个数据分布到另一个数据分布==</p><h2 id="Lifelong-Learning-and-Catastrophic-Forgetting-in-Neural-Networks"><a href="#Lifelong-Learning-and-Catastrophic-Forgetting-in-Neural-Networks" class="headerlink" title="Lifelong Learning and Catastrophic Forgetting in Neural Networks"></a>Lifelong Learning and Catastrophic Forgetting in Neural Networks</h2><h3 id="Lifelong-Machine-Learning"><a href="#Lifelong-Machine-Learning" class="headerlink" title="Lifelong Machine Learning"></a>Lifelong Machine Learning</h3><p>对于连接型的模型，三个主要的方法被提出</p><ol><li>allocating additional neural resources for new knowledge</li></ol><p>为新的知识分配新的额外的神经网络</p><ol><li>using non-overlapping representations if resources are fixed</li></ol><p>如果网络资源固定，则使用不同的部分</p><ol><li>interleaving the old knowledge as the new information is represented</li></ol><p>在新知识获取的时候交叉的加入使用老知识</p><ol><li>Retrain the whole network while regularizing to prevent catastrophic forgetting with previously learned tasks</li><li>Selectively train the network and expand it if necessary to represent new tasks</li><li>model complementary learning systems for memory consolidation by using memory replay to consolidate internal representations</li></ol><hr><h3 id="Regularization-Approaches"><a href="#Regularization-Approaches" class="headerlink" title="Regularization Approaches"></a>Regularization Approaches</h3><p>Regularization approaches alleviate catastrophic forgetting by imposing constraints on the update of the neural weights.</p><p>受突触在更新时，设置几个不同的可塑程度用来保护旧的知识的启发。</p><p>[带*为已整理到ppt中，不带为未整理]</p><p>*(Li&amp;Hoiem(2016))采用知识蒸馏的方法，使得现在的模型和之前的任务中学到的模型较为相似。即从大的，高规格化的模型中将知识转移到一个较小的模型中。</p><p>backward：高依赖于task之间的相关性，并且对于一个task的训练时间和其他task的数目线性相关。其需要一部分old task的数据。</p><p>Jung et al.(2018)提出了使用l2距离来在不同的task中正则化最后的隐藏激活层。</p><p>为保护之前学习到的输入输出对应关系通过使用老task的参数来计算额外的激活层.</p><p>这个计算资源要求高，因为每次计算新的数据抽样都要求了老的task的参数。</p><p>其他的做法有完全防止在训练老task上的参数的更新(Razavian et al. 2014)</p><p>和降低old task对应的参数的学习率在防止模型在使用新的数据进行训练时，其参数发生显著的变化。</p><p>Kirkpatrick et al. (2017)提出了elastic weight consolidation(EWC) model算法，在有监督和强化的场景下。这个方法包括了关于新老任务对应的不同参数的二次惩罚，这样减慢了和task相关的权重的更新。</p><p>backward：Kemker et al.(2018)的实验证明，EWC的方法对不同task中的排列组合训练表现较好，但是不适合于增量学习的场景。</p><hr><h3 id="Dynamic-Architectures"><a href="#Dynamic-Architectures" class="headerlink" title="Dynamic Architectures"></a>Dynamic Architectures</h3><p>…</p><p>Cortes et al.(2016)提出了一个适应性的算法，可以自适应的调整网络的结构和他的权重，通过平衡模型的复杂度和经验风险最小化。</p><p>Xiao et al.(2014) 提出了一个训练算法，网络增量学习在容量上以分级的形式增加不同的程度。网络根据他们的相似度分组并且自治的规划到几个不同的层次。模型从一个网络上继承参数下来以加速训练。这种方法只有最重要的几个layer能得到训练，并且这样的反向传播（分组的网络）计算效率不高。</p><p>Draelos et al.(2017)提出了</p><p>NDL讲了 DEN没讲</p><p>SOINN没讲</p><p>iCaRL可以讲一下</p><p>Parisi, Tani, Weber</p><p>&amp; Wermter (2017) showed that lifelong learning of human action sequences can be achieved in terms of prediction-driven neural dynamics with internal representations emerging in a hierarchy of recurrent self-organizing networks</p><p>GWR model</p><p>每个部分详细的说三篇即可</p>]]></content>
    
    
    <categories>
      
      <category>lifelong</category>
      
      <category>review</category>
      
    </categories>
    
    
    <tags>
      
      <tag>lifelong_learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MBR-2021acl</title>
    <link href="/2021/08/18/MBR-2021acl/"/>
    <url>/2021/08/18/MBR-2021acl/</url>
    
    <content type="html"><![CDATA[<h1 id="Understanding-the-Properties-of-Minimum-Bayes-Risk-Decoding-in-Neural-Machine-Translation"><a href="#Understanding-the-Properties-of-Minimum-Bayes-Risk-Decoding-in-Neural-Machine-Translation" class="headerlink" title="Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation"></a><strong>Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation</strong></h1><p>​    最近看了一篇acl2021的论文，文章详细分析了Eikema和Aziz在coling2020上提出的MBR decoding和NMT当前的一些问题。现在来仔细阅读一番。</p><p>​    文章首先指出，NMT现在存在着一些biases：例如模型产生的翻译往往比标准译文更短，高估词频高的词的概率，并且对copy noise和域偏移的鲁棒性很低。Eikema和Aziz将这些问题归结于beam search并提出了使用MBR(Minimum Bayes Risk)的方法来进行解码。</p><p>​    本文分析了MBR在之前一些biases上的表现，发现由于采用的utility函数的特征。MBR在长度和词频上仍然存在偏差，但是在copy noise和domain shift上仍然提升了鲁棒性。</p><p>​    本文的主要贡献为：</p><ul><li>如果MBR采用的utility function更偏向于短的翻译，那么MBR也继承了这个偏置。</li><li>MBR仍然存在词频的偏置，仍然会高估高频词汇和低估低频词汇的概率。</li><li>与beam search相比，MBR对训练集中存在的copy noise更加稳定。</li><li>MBR比beam search有更好的域鲁棒性，能够比beam search产生更少的hallucinated content（流利但意思与原文不相关的句子）</li></ul><p>NMT目前存在的一些问题：</p><p>1.长度偏置，模型输出的译文往往比参考译文更短。</p><p>2.词频偏置：高估高频词汇的概率，低估低频词汇的概率。</p><p>3.beam search curse:随着beam size的增加，模型的翻译结果反而下降。</p><p>4.对copy-noise的敏感性。</p><p>5.low domain robustness：在不同的域中的稳定性差。</p><p>6.inadequacy of the mode:最大后验概率得到的结果不是最合适的。</p><p>MBR Decoding的定义：</p><p>首先根据模型的输出概率采样，得到一个sample pools（采样是每一次根据模型输出的后验概率进行采样，直到得到终止符为止，重复N次，得到采样池）</p><p>然后从采样池中进行如下公式得到MBR得到的结果：</p><script type="math/tex; mode=display">y^*=\mathop{\arg\max_{\\s_i \in S}}{\frac{1}{n}\sum_{s_j=1}^nu(s_i. s_j)}</script><p>其中u为utility function,例如bleu，metor之类的，采样池中得到一个与其他采样整体比较相似的采样作为MBR的结果，用来表现模型的整体输出。</p><p>接下来是论文的实验和结论部分，beam size为5，除了beam search和MBR，论文还比较了简单抽样的效果，简单抽样使用100次来求每次sample的变化程度。</p><p>下图展示MBR和beam search在out-domain和in-domain的上的CHRF1 scores差值。</p><p><img src="/2021/08/18/MBR-2021acl/figure1.png" alt="figure1"></p><p>可以发现随着sample数目的增加，MBR的效果越来越趋近beam search，在有些情况下甚至超过，并且在out-of-domain的数据中，MBR比beam search的差值要比在in domain数据中的差值更高。因此MBR并不存在beam search curse这类问题。</p><p>文章研究了MBR上使用不同utility function时对模型效果的影响。</p><p>一共使用了如下这些utility function：</p><p><img src="/2021/08/18/MBR-2021acl/table1.png" alt="table1"></p><p>并且对于MBR Decoding而言，没有一个utility function对所有的metrics都是表现最好的，往往与评价指标相关的utility function在对应评价指标下表现的最好。</p><p>如下图所示：</p><p><img src="/2021/08/18/MBR-2021acl/figure2.png" alt="figure2"></p><p>并且，utility function的选择也会影响对应length bias的表现，如下所示：</p><p><img src="/2021/08/18/MBR-2021acl/table2.png" alt="figure2"></p><p>​    由表可见，总体表现最好的指标并不一定是最好的utility function，其中symmetric是作者怀疑是由于指标的单向性，因此将指标改为了：</p><script type="math/tex; mode=display">u_{sym}(s_i, s_j)=H(u(s_i,s_j),u(s_j,s_i))</script><p>采用双向的平滑策略，但是这个也并没有很好的解决length bias的问题。</p><p>​    通过这个实验，作者得出了<strong>MBR inherits length biases associated with its utility function.</strong></p><p>对于Token frequency bias，作者发现，虽然MBR相比beam search有所缓解，但是仍然存在对应的偏置：</p><p><img src="/2021/08/18/MBR-2021acl/figure3.png" alt="figure3"></p><p>横坐标为在Translation中出现的概率，纵坐标为在traning data中出现的概率。</p><p>​    通过和single sample的对比，可以发现sample最能提下模型本身的输出，但是sample出来的句子的metric是最差的。</p><p>在Domain robustness上，MBR得到hallucination的概率更小，作者认为可能是这种句子被MBR assigns a lower utility score。生成的hallucination的概率如下:</p><p><img src="/2021/08/18/MBR-2021acl/figure5.png" alt="figure5"></p><p>使用robustness benchmark of Muller et al(2020)的结果如下：</p><p><img src="/2021/08/18/MBR-2021acl/figure4.png" alt="figure4"></p><p>作者认为可能是MBR减少了hallucinations的产生，从而使得MBR decoding在unknown domain上表现的更好。</p><p>MBR decoding对copy noise更加稳定，在copy noise比重比较小的情况下，beam search和MBR都表现不错，但是当copy noise的比例达到了5~25%时，MBR表现得比beam search好很多（在Arabic-German上达到了超过10的bleu提升）</p><p>文章最后的总结，文章使用了当前常见的metric是作为utility function来研究MBR的特征，发现其仍然存在length bias和token frequency，并且length bias与其utility function紧密相连。但是MBR也解决了一些缺点，比如copy-noise和domain shift，MBR通过使用utility function打分的方式，来使得这些句子在sample pools中不那么容易的被选中，来实现其鲁棒性。研究中，MBR decoding并没有表现出比beam search更好的结果，但是由于MBR的鲁棒性，作者会在更多的效用函数上继续研究，包括一些可训练的效用函数。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>MBR-2021acl</title>
    <link href="/2021/08/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/MBR-2021acl/"/>
    <url>/2021/08/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/MBR-2021acl/</url>
    
    <content type="html"><![CDATA[<h1 id="Understanding-the-Properties-of-Minimum-Bayes-Risk-Decoding-in-Neural-Machine-Translation"><a href="#Understanding-the-Properties-of-Minimum-Bayes-Risk-Decoding-in-Neural-Machine-Translation" class="headerlink" title="Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation"></a><strong>Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation</strong></h1><p>​    最近看了一篇acl2021的论文，文章详细分析了Eikema和Aziz在coling2020上提出的MBR decoding和NMT当前的一些问题。现在来仔细阅读一番。</p><p>​    文章首先指出，NMT现在存在着一些biases：例如模型产生的翻译往往比标准译文更短，高估词频高的词的概率，并且对copy noise和域偏移的鲁棒性很低。Eikema和Aziz将这些问题归结于beam search并提出了使用MBR(Minimum Bayes Risk)的方法来进行解码。</p><p>​    本文分析了MBR在之前一些biases上的表现，发现由于采用的utility函数的特征。MBR在长度和词频上仍然存在偏差，但是在copy noise和domain shift上仍然提升了鲁棒性。</p><p>​    本文的主要贡献为：</p><ul><li>如果MBR采用的utility function更偏向于短的翻译，那么MBR也继承了这个偏置。</li><li>MBR仍然存在词频的偏置，仍然会高估高频词汇和低估低频词汇的概率。</li><li>与beam search相比，MBR对训练集中存在的copy noise更加稳定。</li><li>MBR比beam search有更好的域鲁棒性，能够比beam search产生更少的hallucinated content（流利但意思与原文不相关的句子）</li></ul><p>NMT目前存在的一些问题：</p><p>1.长度偏置，模型输出的译文往往比参考译文更短。</p><p>2.词频偏置：高估高频词汇的概率，低估低频词汇的概率。</p><p>3.beam search curse:随着beam size的增加，模型的翻译结果反而下降。</p><p>4.对copy-noise的敏感性。</p><p>5.low domain robustness：在不同的域中的稳定性差。</p><p>6.inadequacy of the mode:最大后验概率得到的结果不是最合适的。</p><p>MBR Decoding的定义：</p><p>首先根据模型的输出概率采样，得到一个sample pools（采样是每一次根据模型输出的后验概率进行采样，直到得到终止符为止，重复N次，得到采样池）</p><p>然后从采样池中进行如下公式得到MBR得到的结果：</p><script type="math/tex; mode=display">y^*=\mathop{\arg\max_{\\s_i \in S}}{\frac{1}{n}\sum_{s_j=1}^nu(s_i. s_j)}</script><p>其中u为utility function,例如bleu，metor之类的，采样池中得到一个与其他采样整体比较相似的采样作为MBR的结果，用来表现模型的整体输出。</p><p>接下来是论文的实验和结论部分，beam size为5，除了beam search和MBR，论文还比较了简单抽样的效果，简单抽样使用100次来求每次sample的变化程度。</p><p>下图展示MBR和beam search在out-domain和in-domain的上的CHRF1 scores差值。</p><p><img src="/2021/08/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/MBR-2021acl/figure1.png" alt="figure1"></p><p>可以发现随着sample数目的增加，MBR的效果越来越趋近beam search，在有些情况下甚至超过，并且在out-of-domain的数据中，MBR比beam search的差值要比在in domain数据中的差值更高。因此MBR并不存在beam search curse这类问题。</p><p>文章研究了MBR上使用不同utility function时对模型效果的影响。</p><p>一共使用了如下这些utility function：</p><p><img src="/2021/08/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/MBR-2021acl/table1.png" alt="table1"></p><p>并且对于MBR Decoding而言，没有一个utility function对所有的metrics都是表现最好的，往往与评价指标相关的utility function在对应评价指标下表现的最好。</p><p>如下图所示：</p><p><img src="/2021/08/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/MBR-2021acl/figure2.png" alt="figure2"></p><p>并且，utility function的选择也会影响对应length bias的表现，如下所示：</p><p><img src="/2021/08/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/MBR-2021acl/table2.png" alt="figure2"></p><p>​    由表可见，总体表现最好的指标并不一定是最好的utility function，其中symmetric是作者怀疑是由于指标的单向性，因此将指标改为了：</p><script type="math/tex; mode=display">u_{sym}(s_i, s_j)=H(u(s_i,s_j),u(s_j,s_i))</script><p>采用双向的平滑策略，但是这个也并没有很好的解决length bias的问题。</p><p>​    通过这个实验，作者得出了<strong>MBR inherits length biases associated with its utility function.</strong></p><p>对于Token frequency bias，作者发现，虽然MBR相比beam search有所缓解，但是仍然存在对应的偏置：</p><p><img src="/2021/08/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/MBR-2021acl/figure3.png" alt="figure3"></p><p>横坐标为在Translation中出现的概率，纵坐标为在traning data中出现的概率。</p><p>​    通过和single sample的对比，可以发现sample最能提下模型本身的输出，但是sample出来的句子的metric是最差的。</p><p>在Domain robustness上，MBR得到hallucination的概率更小，作者认为可能是这种句子被MBR assigns a lower utility score。生成的hallucination的概率如下:</p><p><img src="/2021/08/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/MBR-2021acl/figure5.png" alt="figure5"></p><p>使用robustness benchmark of Muller et al(2020)的结果如下：</p><p><img src="/2021/08/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/MBR-2021acl/figure4.png" alt="figure4"></p><p>作者认为可能是MBR减少了hallucinations的产生，从而使得MBR decoding在unknown domain上表现的更好。</p><p>MBR decoding对copy noise更加稳定，在copy noise比重比较小的情况下，beam search和MBR都表现不错，但是当copy noise的比例达到了5~25%时，MBR表现得比beam search好很多（在Arabic-German上达到了超过10的bleu提升）</p><p>文章最后的总结，文章使用了当前常见的metric是作为utility function来研究MBR的特征，发现其仍然存在length bias和token frequency，并且length bias与其utility function紧密相连。但是MBR也解决了一些缺点，比如copy-noise和domain shift，MBR通过使用utility function打分的方式，来使得这些句子在sample pools中不那么容易的被选中，来实现其鲁棒性。研究中，MBR decoding并没有表现出比beam search更好的结果，但是由于MBR的鲁棒性，作者会在更多的效用函数上继续研究，包括一些可训练的效用函数。</p>]]></content>
    
    
    <categories>
      
      <category>NMT</category>
      
      <category>MBR</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NMT, MBR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的第一篇博客文章</title>
    <link href="/2021/08/07/%E5%8D%9A%E5%AE%A2/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%E6%96%87%E7%AB%A0/"/>
    <url>/2021/08/07/%E5%8D%9A%E5%AE%A2/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h2><p>内容</p><h2 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h2><p>内容</p><hr><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
    
    
    <categories>
      
      <category>Begin</category>
      
      <category>blog_learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2021/08/07/%E5%8D%9A%E5%AE%A2/hello-world/"/>
    <url>/2021/08/07/%E5%8D%9A%E5%AE%A2/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
